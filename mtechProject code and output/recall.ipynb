{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'everi time i imagin someon i love i could contact serious ill even death'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json as j\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import csv\n",
    "\n",
    "data = pd.read_csv('iseardataset.csv')\n",
    "test_data = data\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "words = stopwords.words(\"english\")\n",
    "\n",
    "data['cleaned'] = data['text'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "test_data['cleaned'] = data['text'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "\n",
    "data['cleaned'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.35215 , -0.35603 ,  0.25708 , -0.10611 , -0.20718 ,  0.63596 ,\n",
       "       -1.0129  , -0.45964 , -0.48749 , -0.080555,  0.43769 ,  0.46046 ,\n",
       "       -0.80943 , -0.23336 ,  0.46623 , -0.10866 , -0.1221  , -0.63544 ,\n",
       "       -0.73486 , -0.24848 ,  0.4317  ,  0.092264,  0.52033 , -0.46784 ,\n",
       "        0.016798, -1.5124  , -0.19986 , -0.43351 , -0.59247 ,  0.18088 ,\n",
       "        3.5194  , -0.7024  ,  0.23613 , -0.68514 , -0.37009 , -0.080451,\n",
       "        0.10635 , -0.085495, -0.18451 ,  0.29771 ,  0.18123 ,  0.53627 ,\n",
       "       -0.1001  , -0.55165 ,  0.098833, -0.12942 , -0.82628 , -0.4329  ,\n",
       "       -0.10301 , -0.56079 ], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "SeqLength = 42\n",
    "counter = 0\n",
    "for p in wordsList:\n",
    "    counter = counter+1\n",
    "    if counter >= SeqLength:                \n",
    "        break\n",
    "        \n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "wordVectors[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeltext = data['label']\n",
    "\n",
    "labeltext1= data['label']\n",
    "\n",
    "\n",
    "testdata = data['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueLabel=[]\n",
    "for w in labeltext:\n",
    "    if w not in uniqueLabel:\n",
    "        uniqueLabel.append(w)\n",
    "        \n",
    "uniqueLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7516"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter=[]\n",
    "for word in uniqueLabel:\n",
    "    count=0\n",
    "    for word1 in labeltext:\n",
    "        if word==word1:\n",
    "            count=count+1\n",
    "    counter.append(count)\n",
    "    \n",
    "sum(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on day i feel close partner friend when i feel peac also experi close contact peopl i regard great'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = data['cleaned']\n",
    "\n",
    "\n",
    "len(list1)\n",
    "\n",
    "\n",
    "list1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files is 7516\n",
      "The total number of words in the files is 91532\n",
      "The average number of words in the files is 12.178286322511974\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "numWords=[]\n",
    "x = []\n",
    "maxLengthWords = 0\n",
    "for pf in list1:\n",
    "    counter = len(pf.split())\n",
    "    x.append(counter)\n",
    "    numWords.append(counter)\n",
    "    if(maxLengthWords < counter):\n",
    "        maxLengthWords = counter\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))\n",
    "print(maxLengthWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter = 0\n",
    "for pf in list1:\n",
    "    indexCounter = 0\n",
    "    cleanedLine = cleanSentences(pf)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        try:\n",
    "            ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "        indexCounter = indexCounter + 1\n",
    "        if indexCounter >= maxSeqLength:\n",
    "            break\n",
    "    fileCounter = fileCounter + 1 \n",
    "    \n",
    "np.save('idsMatrixRecall', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter = 0\n",
    "for pf in testdata:\n",
    "    indexCounter = 0\n",
    "    cleanedLine = cleanSentences(pf)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        try:\n",
    "            ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "        indexCounter = indexCounter + 1\n",
    "        if indexCounter >= maxSeqLength:\n",
    "            break\n",
    "    fileCounter = fileCounter + 1 \n",
    "    \n",
    "np.save('idsMatrixRecallTest', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    61,     41,    998,   6710, 399999,     41,  90634,    530,\n",
       "           41,   2716, 399999,  86821,   1927,   1671,     79, 399999,\n",
       "        11056,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids1 = np.load('idsMatrixRecall.npy')\n",
    "\n",
    "\n",
    "ids1[41]\n",
    "\n",
    "\n",
    "ids = np.load('idsMatrixRecallTest.npy')\n",
    "ids[0]\n",
    "\n",
    "ids[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,7515)\n",
    "        labeltxt = labeltext[num]\n",
    "        if labeltxt==\"joy\":\n",
    "            labels.append([1,0,0,0,0,0,0])\n",
    "        if labeltxt==\"fear\":\n",
    "            labels.append([0,1,0,0,0,0,0])\n",
    "        if labeltxt==\"anger\":\n",
    "            labels.append([0,0,1,0,0,0,0])\n",
    "        if labeltxt==\"sadness\":\n",
    "            labels.append([0,0,0,1,0,0,0])\n",
    "        if labeltxt==\"disgust\":\n",
    "            labels.append([0,0,0,0,1,0,0])\n",
    "        if labeltxt==\"shame\":\n",
    "            labels.append([0,0,0,0,0,1,0])\n",
    "        if labeltxt==\"guilt\":\n",
    "            labels.append([0,0,0,0,0,0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,7515)\n",
    "        labeltxt = labeltext1[num]\n",
    "        if labeltxt==\"joy\":\n",
    "            labels.append([1,0,0,0,0,0,0])\n",
    "        if labeltxt==\"fear\":\n",
    "            labels.append([0,1,0,0,0,0,0])\n",
    "        if labeltxt==\"anger\":\n",
    "            labels.append([0,0,1,0,0,0,0])\n",
    "        if labeltxt==\"sadness\":\n",
    "            labels.append([0,0,0,1,0,0,0])\n",
    "        if labeltxt==\"disgust\":\n",
    "            labels.append([0,0,0,0,1,0,0])\n",
    "        if labeltxt==\"shame\":\n",
    "            labels.append([0,0,0,0,0,1,0])\n",
    "        if labeltxt==\"guilt\":\n",
    "            labels.append([0,0,0,0,0,0,1])\n",
    "        arr[i] = ids1[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "lstmUnits = 10\n",
    "numClasses = 7\n",
    "iterations = 100000\n",
    "numDimensions = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(64), Dimension(100), Dimension(50)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(64, 7), dtype=float32)\n",
      "Tensor(\"Equal:0\", shape=(64,), dtype=bool)\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-18-24b8ea197ad3>:17: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "print(prediction)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "print(correctPred)\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "print(accuracy)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to modelsRecall/pretrained_lstm.ckpt-10000\n",
      "saved to modelsRecall/pretrained_lstm.ckpt-20000\n",
      "saved to modelsRecall/pretrained_lstm.ckpt-30000\n",
      "saved to modelsRecall/pretrained_lstm.ckpt-40000\n",
      "saved to modelsRecall/pretrained_lstm.ckpt-50000\n",
      "saved to modelsRecall/pretrained_lstm.ckpt-60000\n",
      "saved to modelsRecall/pretrained_lstm.ckpt-70000\n",
      "saved to modelsRecall/pretrained_lstm.ckpt-80000\n",
      "saved to modelsRecall/pretrained_lstm.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "   nextBatch, nextBatchLabels = getTrainBatch()\n",
    "   sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "   #Write summary to Tensorboard\n",
    "   if (i % 50 == 0):\n",
    "       summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "       writer.add_summary(summary, i)\n",
    "\n",
    "   #Save the network every 10,000 training iterations\n",
    "   if (i % 10000 == 0 and i != 0):\n",
    "       save_path = saver.save(sess, \"modelsRecall/pretrained_lstm.ckpt\", global_step=i)\n",
    "       print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from modelsRecall\\pretrained_lstm.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('modelsRecall'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 31.25\n",
      "Accuracy for this batch: 29.6875\n",
      "Accuracy for this batch: 43.75\n",
      "Accuracy for this batch: 39.0625\n",
      "Accuracy for this batch: 28.125\n",
      "Accuracy for this batch: 31.25\n",
      "Accuracy for this batch: 35.9375\n",
      "Accuracy for this batch: 29.6875\n",
      "Accuracy for this batch: 29.6875\n",
      "Accuracy for this batch: 35.9375\n",
      "Accuracy for this batch: 31.25\n",
      "Accuracy for this batch: 28.125\n",
      "Accuracy for this batch: 28.125\n",
      "Accuracy for this batch: 40.625\n",
      "Accuracy for this batch: 34.375\n",
      "Accuracy for this batch: 31.25\n",
      "Accuracy for this batch: 35.9375\n",
      "Accuracy for this batch: 34.375\n",
      "Accuracy for this batch: 35.9375\n",
      "Accuracy for this batch: 42.1875\n",
      "Accuracy for this batch: 42.1875\n",
      "Accuracy for this batch: 43.75\n",
      "Accuracy for this batch: 31.25\n",
      "Accuracy for this batch: 34.375\n",
      "Accuracy for this batch: 31.25\n",
      "Accuracy for this batch: 42.1875\n",
      "Accuracy for this batch: 32.8125\n",
      "Accuracy for this batch: 23.4375\n",
      "Accuracy for this batch: 42.1875\n",
      "Accuracy for this batch: 43.75\n",
      "Accuracy for this batch: 26.5625\n",
      "Accuracy for this batch: 29.6875\n",
      "Accuracy for this batch: 31.25\n",
      "Accuracy for this batch: 45.3125\n",
      "Accuracy for this batch: 31.25\n",
      "Accuracy for this batch: 40.625\n",
      "Accuracy for this batch: 31.25\n",
      "Accuracy for this batch: 29.6875\n",
      "Accuracy for this batch: 34.375\n",
      "Accuracy for this batch: 31.25\n",
      "Accuracy for this batch: 28.125\n",
      "Accuracy for this batch: 34.375\n",
      "Accuracy for this batch: 37.5\n",
      "Accuracy for this batch: 35.9375\n",
      "Accuracy for this batch: 32.8125\n",
      "Accuracy for this batch: 26.5625\n",
      "Accuracy for this batch: 34.375\n",
      "Accuracy for this batch: 29.6875\n",
      "Accuracy for this batch: 48.4375\n",
      "Accuracy for this batch: 35.9375\n"
     ]
    }
   ],
   "source": [
    "iterations = 50\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"When I was not accepted as a student in finance and accounting.\"\n",
    "inputMatrix = getSentenceMatrix(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "joy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.4158468 ,  0.03969044, -0.48083097, -0.614553  ,  0.7445061 ,\n",
       "       -1.1502123 ,  0.71592015], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "max1 = 0\n",
    "count = 0\n",
    "for i in range(0,6):\n",
    "    if predictedSentiment[max1] < predictedSentiment[i]:\n",
    "        max1 = i\n",
    "        count = predictedSentiment[i]\n",
    "\n",
    "print(max1,count)\n",
    "print(uniqueLabel[max1])\n",
    "predictedSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"When one lets friends down\"\n",
    "inputMatrix = getSentenceMatrix(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.6279112\n",
      "shame\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.31104717,  0.4194594 , -0.29769546,  0.5929631 , -0.26720387,\n",
       "        0.6279112 ,  0.3004165 ], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "max1 = 0\n",
    "count = 0\n",
    "for i in range(0,6):\n",
    "    if predictedSentiment[max1] < predictedSentiment[i]:\n",
    "        max1 = i\n",
    "        count = predictedSentiment[i]\n",
    "\n",
    "print(max1,count)\n",
    "print(uniqueLabel[max1])\n",
    "predictedSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"When the whole family gets together for a one week holiday.\"\n",
    "inputMatrix = getSentenceMatrix(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "joy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.3058782 , -0.34141076,  0.472448  , -0.6915202 ,  0.8651525 ,\n",
       "       -0.4371509 , -0.6871174 ], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "max1 = 0\n",
    "count = 0\n",
    "for i in range(0,6):\n",
    "    if predictedSentiment[max1] < predictedSentiment[i]:\n",
    "        max1 = i\n",
    "        count = predictedSentiment[i]\n",
    "\n",
    "print(max1,count)\n",
    "print(uniqueLabel[max1])\n",
    "predictedSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"On days when I feel close to my partner and other friends. When I feel at peace with myself and also experience a close contact with people whom I regard greatly.\"\n",
    "inputMatrix = getSentenceMatrix(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 100 is out of bounds for axis 1 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-fc84b3e2f058>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputTextArray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0minputText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0minputMatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetSentenceMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mpredictedSentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputMatrix\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-b5cce342fd34>\u001b[0m in \u001b[0;36mgetSentenceMatrix\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindexCounter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0msentenceMatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindexCounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordsList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0msentenceMatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindexCounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m399999\u001b[0m \u001b[1;31m#Vector for unkown words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 100 is out of bounds for axis 1 with size 100"
     ]
    }
   ],
   "source": [
    "inputArray = pd.read_csv('iseardataset.csv')\n",
    "inputTextArray = inputArray[\"text\"]\n",
    "predictedArray=[]\n",
    "inputLength = len(inputTextArray)\n",
    "i=0\n",
    "for text in inputTextArray:\n",
    "    inputText = text\n",
    "    inputMatrix = getSentenceMatrix(inputText)\n",
    "    i=i+1\n",
    "    predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "    max1 = 0\n",
    "    count = 0\n",
    "    for j in range(0,6):\n",
    "        if predictedSentiment[max1] < predictedSentiment[j]:\n",
    "            max1 = j\n",
    "            count = predictedSentiment[j]\n",
    "    predictedArray.append(uniqueLabel[max1])\n",
    "    if(i>=inputLength):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2342\n"
     ]
    }
   ],
   "source": [
    "predictedArray[0]\n",
    "print(len(predictedArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports: \n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2342, 7516]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-edf4e6518592>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictedArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputArray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2342, 7516]"
     ]
    }
   ],
   "source": [
    "y_true = predictedArray\n",
    "y_pred = inputArray['label']\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
