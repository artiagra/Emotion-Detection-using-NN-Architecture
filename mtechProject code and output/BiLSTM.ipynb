{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "---------------------------------------\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "SeqLength = 42\n",
    "counter = 0\n",
    "for p in wordsList:\n",
    "    counter = counter+1\n",
    "    if counter >= SeqLength:                \n",
    "        break\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.039495,  1.5316  , -0.39992 , -0.20091 ,  1.3784  ,  0.23264 ,\n",
       "        0.40811 ,  0.09424 ,  0.80493 ,  1.189   ,  0.3709  ,  0.2923  ,\n",
       "        0.051458, -0.91471 ,  0.79913 ,  0.031613,  0.23426 , -0.078071,\n",
       "        0.55265 , -0.094033, -0.067781,  0.94492 ,  0.13196 , -0.2352  ,\n",
       "        1.6138  , -0.31953 , -1.3038  , -0.33394 ,  1.0244  ,  0.12196 ,\n",
       "        1.5858  ,  1.0825  , -0.15639 , -0.38267 , -0.76347 , -0.49954 ,\n",
       "       -0.5254  , -0.33106 ,  0.29729 , -0.17441 , -0.076043, -0.83093 ,\n",
       "       -0.47857 , -0.029247, -0.26329 , -0.3085  ,  0.15287 , -0.30547 ,\n",
       "        0.15899 , -0.30671 ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joyIndex = wordsList.index('happiness')\n",
    "print(joyIndex)\n",
    "wordVectors[joyIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[ 9811  1655  3946 13866 18741  2661  5876     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "maxSeqLength = 10 #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "inputSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "inputSentence[0] = wordsList.index(\"happiness\")\n",
    "inputSentence[1] = wordsList.index(\"fear\")\n",
    "inputSentence[2] = wordsList.index(\"anger\")\n",
    "inputSentence[3] = wordsList.index(\"sadness\")\n",
    "inputSentence[4] = wordsList.index(\"disgust\")\n",
    "inputSentence[5] = wordsList.index(\"surprise\")\n",
    "inputSentence[6] = wordsList.index(\"neutral\")\n",
    "#inputSentence[7],inputSentence[8] and inputSentence[9] are going to be 0\n",
    "print(inputSentence.shape)\n",
    "print(inputSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.039495   1.5316    -0.39992   -0.20091    1.3784     0.23264\n",
      "   0.40811    0.09424    0.80493    1.189      0.3709     0.2923\n",
      "   0.051458  -0.91471    0.79913    0.031613   0.23426   -0.078071\n",
      "   0.55265   -0.094033  -0.067781   0.94492    0.13196   -0.2352\n",
      "   1.6138    -0.31953   -1.3038    -0.33394    1.0244     0.12196\n",
      "   1.5858     1.0825    -0.15639   -0.38267   -0.76347   -0.49954\n",
      "  -0.5254    -0.33106    0.29729   -0.17441   -0.076043  -0.83093\n",
      "  -0.47857   -0.029247  -0.26329   -0.3085     0.15287   -0.30547\n",
      "   0.15899   -0.30671  ]\n",
      " [ 0.53661   -0.33872    0.28772   -0.72584   -0.041209   0.28498\n",
      "   0.21665    0.62408   -0.1204     0.41135    0.10111   -0.11545\n",
      "  -0.15652   -0.39265    0.66286    0.71563    0.4214    -0.37041\n",
      "  -0.081016  -0.2901    -0.28407    0.30707    0.035974   0.32002\n",
      "   0.45456   -2.083     -0.6589    -0.081918   1.0431     0.50839\n",
      "   2.5433     0.81832   -0.11508   -1.1471    -1.0422    -0.22972\n",
      "  -0.68159   -1.3633     0.063994  -0.27115   -0.41006    0.090468\n",
      "   0.19801    0.486      0.84081    0.38655   -0.40763    0.20565\n",
      "  -0.1169    -0.55731  ]\n",
      " [-0.044806   0.24917    0.088818  -0.7982     0.15031    0.6785\n",
      "   0.83496    0.86424   -0.71302    0.75296   -0.31585   -0.2964\n",
      "  -0.12553   -0.73859    0.1535     0.18059   -0.080169   0.0029328\n",
      "  -0.14163   -0.56985    0.38888    0.6309     0.038375  -0.32777\n",
      "   0.72166   -1.9699     0.081391   0.55689    0.73193    0.83258\n",
      "   2.0362     1.1322     0.31967   -1.1327    -1.3813    -0.30716\n",
      "  -0.77535   -1.5439     0.02619   -0.067238   0.018815   0.47666\n",
      "  -0.80437    0.13336    1.0504     0.28345   -0.54026    0.3938\n",
      "  -0.3018    -0.91219  ]\n",
      " [ 0.38248    1.3166    -0.67796   -0.23382    0.92055    0.39705\n",
      "   1.0954     1.3475    -0.33678    0.71333    0.055274   0.25823\n",
      "  -0.14289   -0.72494    0.60271    0.048186  -0.55067   -0.0056619\n",
      "  -0.015116  -0.679     -0.52649    1.4572     0.44992   -0.70761\n",
      "   1.561     -0.41428   -1.1798     0.98563    1.0045     0.8048\n",
      "   1.1182     1.3909     0.70073   -0.49185   -0.9694    -0.32769\n",
      "  -0.34098   -1.3959     0.67482    0.13914    0.18353    0.25068\n",
      "  -0.92972   -0.28589    0.27042    0.84941   -0.44641    0.50314\n",
      "  -0.37851   -0.65623  ]\n",
      " [-0.55735    0.088292  -0.34505   -0.5124     0.33486    0.50618\n",
      "   0.90091    0.87638   -0.55048    0.3189    -0.14744   -0.022987\n",
      "  -0.32802   -0.22225    0.023042  -0.26524   -0.53355    0.013756\n",
      "  -0.098625  -0.88335   -0.018759   0.90394    0.2552    -0.48816\n",
      "   0.85611   -1.1937    -0.19683    1.0503     0.4081     0.23352\n",
      "   0.52848    1.1698     0.28227   -0.8829    -1.5182    -0.15557\n",
      "  -0.37392   -1.0123     0.38461   -0.21142    0.36867    0.26028\n",
      "  -0.9847     0.6882     0.38737    0.090635  -0.22709    0.25281\n",
      "   0.23433   -1.0114   ]\n",
      " [ 0.43138    0.2556    -0.0054598  0.2672     0.41846   -0.30282\n",
      "  -0.42898    1.2554    -0.37419    0.42207   -0.38164   -0.45101\n",
      "  -0.24843   -0.18889    0.85612   -0.15808    0.13082   -0.77788\n",
      "  -1.0247    -0.47486    0.054011  -0.1516    -0.18829   -0.36845\n",
      "   0.44512   -1.571     -0.55777    0.21816    0.26315    0.18359\n",
      "   2.0894     0.68849   -0.16336   -0.1216     0.038419  -0.302\n",
      "   0.06687    0.21847   -0.47899   -1.1706     0.093452   0.28611\n",
      "  -0.55597   -0.65429    0.66782    0.04971   -0.0052173  0.7006\n",
      "   0.32465    0.24523  ]\n",
      " [ 0.10193   -0.31175   -0.32207   -0.069121   0.83464   -0.19136\n",
      "   0.61143   -0.048183  -0.569     -0.31074    1.0109     0.41018\n",
      "   0.07679    0.53938   -0.35247    1.0868     0.87827   -0.15841\n",
      "  -0.28287   -0.71635   -0.050837  -0.024865   0.73021    0.60965\n",
      "  -0.748     -0.63331    0.59424    0.89076    0.21573    0.68681\n",
      "   2.047     -0.46433   -0.081564  -0.84879   -0.21514   -0.77163\n",
      "  -0.18683   -0.16663   -0.51517   -0.67673    0.46657   -0.33409\n",
      "   0.72181    0.15686   -0.52404   -0.22546   -0.23131    0.32637\n",
      "   0.30859    0.54118  ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.       ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.       ]\n",
      " [ 0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.       ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,inputSentence).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# file = pd.read_csv('iseardataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file['text'][1606]\n",
    "# file['label'][1606]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textList = file['text']\n",
    "# textList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numWords = []\n",
    "# count=0\n",
    "# for pf in textList:\n",
    "#     counter = len(pf.split())\n",
    "#     numWords.append(counter)\n",
    "#     if(counter > 180):\n",
    "#         count=count+1\n",
    "    \n",
    "# print('files read finished')\n",
    "\n",
    "# numFiles = len(numWords)\n",
    "# print('The total number of files is', numFiles)\n",
    "# print('The total number of words in the files is', sum(numWords))\n",
    "# print('The average number of words in the files is', sum(numWords)/len(numWords))\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file1 = pd.read_csv('Data/text_emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelledColumn = file1['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = labelledColumn.replace(\"worry\",\"sadness\")\n",
    "ll = ll.replace(\"love\",\"happiness\")\n",
    "ll = ll.replace(\"fun\",\"happiness\")\n",
    "ll = ll.replace(\"enthusiasm\",\"happiness\")\n",
    "ll = ll.replace(\"empty\",\"sadness\")\n",
    "ll = ll.replace(\"boredom\",\"neutral\")\n",
    "ll = ll.replace(\"relief\",\"happiness\")\n",
    "ll = ll.replace(\"hate\",\"anger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sadness', 'happiness', 'neutral', 'surprise', 'anger']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueWord1 = []\n",
    "for word in ll:\n",
    "    if word not in uniqueWord1:\n",
    "        uniqueWord1.append(word)\n",
    "        \n",
    "uniqueWord1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14451, 13112, 8817, 2187, 1433]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter1 = []\n",
    "for word in uniqueWord1:\n",
    "    count = 0\n",
    "    for word1 in ll:\n",
    "        if word == word1:\n",
    "            count=count+1\n",
    "    counter1.append(count)\n",
    "\n",
    "counter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "textList1 = file1['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files read finished\n",
      "The total number of files is 40000\n",
      "The total number of words in the files is 528675\n",
      "The average number of words in the files is 13.216875\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "numWords1 = []\n",
    "count1=0\n",
    "for pf in textList1:\n",
    "    counter1 = len(pf.split())\n",
    "    numWords1.append(counter1)\n",
    "    if(counter1 > 40):\n",
    "        count1=count1+1\n",
    "    \n",
    "print('files read finished')\n",
    "\n",
    "numFiles1 = len(numWords1)\n",
    "print('The total number of files is', numFiles1)\n",
    "print('The total number of words in the files is', sum(numWords1))\n",
    "print('The average number of words in the files is', sum(numWords1)/len(numWords1))\n",
    "print(count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"\\n\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "# pf = textList[0]\n",
    "# indexCounter = 0\n",
    "# cleanedLine = cleanSentences(pf)\n",
    "# split = cleanedLine.split()\n",
    "# for word in split:\n",
    "#     try:\n",
    "#         firstFile[indexCounter] = wordsList.index(word)\n",
    "#     except ValueError:\n",
    "#         firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "#     indexCounter = indexCounter + 1\n",
    "# firstFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelPrediction = [\"happiness\",\"fear\",\"anger\",\"sadness\",\"surprise\",\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numFiles = numFiles + numFiles1\n",
    "# ids = np.zeros((numFiles1, maxSeqLength), dtype='int32')\n",
    "# fileCounter = 0\n",
    "# for pf in textList:\n",
    "#     indexCounter = 0\n",
    "#     cleanedLine = cleanSentences(pf)\n",
    "#     split = cleanedLine.split()\n",
    "#     for word in split:\n",
    "#         try:\n",
    "#             ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#         except ValueError:\n",
    "#             ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#         indexCounter = indexCounter + 1\n",
    "#         if indexCounter >= maxSeqLength:\n",
    "#             break\n",
    "#     fileCounter = fileCounter + 1 \n",
    "    \n",
    "# for pf in textList1:\n",
    "#     indexCounter = 0\n",
    "#     cleanedLine = cleanSentences(pf)\n",
    "#     split = cleanedLine.split()\n",
    "#     for word in split:\n",
    "#         try:\n",
    "#             ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#         except ValueError:\n",
    "#             ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#         indexCounter = indexCounter + 1\n",
    "#         if indexCounter >= maxSeqLength:\n",
    "#             break\n",
    "#     fileCounter = fileCounter + 1 \n",
    "# np.save('idsMatrixFile1', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([399999, 399999,    738,  93325,     81,    242,      4,  22152,\n",
       "         1832,    192,  34140,  93325,    117,    192,   1095,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = np.load('idsMatrixFile1.npy')\n",
    "ids[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399999 399999    738  93325     81    242      4  22152   1832    192\n",
      "  34140  93325    117    192   1095      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n",
      "Re-pinging @ghostridah14: why didn't you go to prom? BC my bf didn't like my friends\n"
     ]
    }
   ],
   "source": [
    "print(ids[5])\n",
    "print(textList1[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 40)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import randint\n",
    "\n",
    "# labels = []\n",
    "# batchSize =24\n",
    "# arr = np.zeros([batchSize, maxSeqLength])\n",
    "# for i in range(batchSize):\n",
    "#      num = randint(1,39999)\n",
    "#      print(num)\n",
    "#      labeltxt = ll[num]\n",
    "#      print(labeltxt)\n",
    "#      labelemo = textList1[num]\n",
    "#      print(labelemo)\n",
    "#      labelindex = wordsList.index(labeltxt)\n",
    "#      print(labelindex)\n",
    "#      arr[i] = ids[num-1:num]\n",
    "#      print(arr[i].shape)\n",
    "#      if labeltxt==\"happiness\":\n",
    "#         labels.append([1,0,0,0,0,0])\n",
    "#      elif labeltxt==\"fear\":\n",
    "#         labels.append([0,1,0,0,0,0])\n",
    "#      elif labeltxt==\"anger\":\n",
    "#         labels.append([0,0,1,0,0,0])\n",
    "#      elif labeltxt==\"sadness\":\n",
    "#         labels.append([0,0,0,1,0,0])\n",
    "#      elif labeltxt==\"surprise\":\n",
    "#         labels.append([0,0,0,0,1,0])\n",
    "#      elif labeltxt==\"neutral\":\n",
    "#         labels.append([0,0,0,0,0,1])\n",
    "#      print(labels[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,29999)\n",
    "        labeltxt = ll[num]\n",
    "        labelindex = wordsList.index(labeltxt)\n",
    "        arr[i] = ids[num-1:num]\n",
    "        if labeltxt==\"happiness\":\n",
    "            labels.append([1,0,0,0,0,0])\n",
    "        elif labeltxt==\"fear\":\n",
    "            labels.append([0,1,0,0,0,0])\n",
    "        elif labeltxt==\"anger\":\n",
    "            labels.append([0,0,1,0,0,0])\n",
    "        elif labeltxt==\"sadness\":\n",
    "            labels.append([0,0,0,1,0,0])\n",
    "        elif labeltxt==\"surprise\":\n",
    "            labels.append([0,0,0,0,1,0])\n",
    "        elif labeltxt==\"neutral\":\n",
    "            labels.append([0,0,0,0,0,1])\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(30000,39999)\n",
    "        #labeltxt = file['label'][num]\n",
    "        labeltxt = ll[num]\n",
    "        labelindex = wordsList.index(labeltxt)\n",
    "        arr[i] = ids[num-1:num]\n",
    "        if labeltxt==\"happiness\":\n",
    "            labels.append([1,0,0,0,0,0])\n",
    "        elif labeltxt==\"fear\":\n",
    "            labels.append([0,1,0,0,0,0])\n",
    "        elif labeltxt==\"anger\":\n",
    "            labels.append([0,0,1,0,0,0])\n",
    "        elif labeltxt==\"sadness\":\n",
    "            labels.append([0,0,0,1,0,0])\n",
    "        elif labeltxt==\"surprise\":\n",
    "            labels.append([0,0,0,0,1,0])\n",
    "        elif labeltxt==\"neutral\":\n",
    "            labels.append([0,0,0,0,0,1])\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 6\n",
    "iterations = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(24), Dimension(40), Dimension(50)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_fw = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "# cell_fw = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "# cell_bw = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "# cell_bw = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "# value, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, data, dtype=tf.float32)\n",
    "\n",
    "lstm_fw_cell = rnn.BasicLSTMCell(lstmUnits, forget_bias=1.0)\n",
    "lstm_bw_cell = rnn.BasicLSTMCell(lstmUnits, forget_bias=1.0)\n",
    "try:\n",
    "    outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32)\n",
    "except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension must be 4 but is 3 for 'transpose_3' (op: 'Transpose') with input shapes: [2,24,40,24], [3].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    687\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimension must be 4 but is 3 for 'transpose_3' (op: 'Transpose') with input shapes: [2,24,40,24], [3].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-880e48d24336>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlstmUnits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumClasses\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[1;34m(a, perm, name, conjugate)\u001b[0m\n\u001b[0;32m   1390\u001b[0m           \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtranspose_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[1;34m(x, perm, name)\u001b[0m\n\u001b[0;32m   7685\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7686\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 7687\u001b[1;33m         \"Transpose\", x=x, perm=perm, name=name)\n\u001b[0m\u001b[0;32m   7688\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7689\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   3160\u001b[0m         op_def=op_def)\n\u001b[0;32m   3161\u001b[0m     self._create_op_helper(ret, compute_shapes=compute_shapes,\n\u001b[1;32m-> 3162\u001b[1;33m                            compute_device=compute_device)\n\u001b[0m\u001b[0;32m   3163\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[1;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   3206\u001b[0m     \u001b[1;31m# compute_shapes argument.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3207\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3208\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3209\u001b[0m     \u001b[1;31m# TODO(b/XXXX): move to Operation.__init__ once _USE_C_API flag is removed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3210\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2425\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs_c_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2426\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2427\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_set_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2398\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2400\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2401\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2402\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2329\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2330\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2332\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, require_shape_fn)\u001b[0m\n\u001b[0;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m                                   require_shape_fn)\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimension must be 4 but is 3 for 'transpose_3' (op: 'Transpose') with input shapes: [2,24,40,24], [3]."
     ]
    }
   ],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "   #Next Batch of reviews\n",
    "   nextBatch, nextBatchLabels = getTrainBatch();\n",
    "   sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "   #Write summary to Tensorboard\n",
    "   if (i % 50 == 0):\n",
    "       summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "       writer.add_summary(summary, i)\n",
    "\n",
    "   #Save the network every 10,000 training iterations\n",
    "   if (i % 10000 == 0 and i != 0):\n",
    "       save_path = saver.save(sess, \"BiLSTM/pretrained_lstm.ckpt\", global_step=i)\n",
    "       print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from file1Models\\pretrained_lstm.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('BiLSTM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 20.8333328366\n",
      "Accuracy for this batch: 25.0\n",
      "Accuracy for this batch: 29.1666656733\n",
      "Accuracy for this batch: 54.1666686535\n",
      "Accuracy for this batch: 29.1666656733\n",
      "Accuracy for this batch: 58.3333313465\n",
      "Accuracy for this batch: 25.0\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 33.3333343267\n",
      "Accuracy for this batch: 41.6666656733\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unknown words\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"so sad\"\n",
    "inputMatrix = getSentenceMatrix(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "\n",
    "# print(\"happiness = \",predictedSentiment[0])\n",
    "# print(\"fear=\",predictedSentiment[1])\n",
    "# print(\"anger=\",predictedSentiment[2])\n",
    "# print(\"sadness=\",predictedSentiment[3])\n",
    "# print(\"surprise=\",predictedSentiment[4])\n",
    "# print(\"neutral=\",predictedSentiment[5])\n",
    "\n",
    "import operator\n",
    "index, value = max(enumerate(predictedSentiment), key=operator.itemgetter(1))\n",
    "\n",
    "labelPrediction[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"i hate you\"\n",
    "inputMatrix = getSentenceMatrix(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "\n",
    "# print(\"happiness = \",predictedSentiment[0])\n",
    "# print(\"fear=\",predictedSentiment[1])\n",
    "# print(\"anger=\",predictedSentiment[2])\n",
    "# print(\"sadness=\",predictedSentiment[3])\n",
    "# print(\"surprise=\",predictedSentiment[4])\n",
    "# print(\"neutral=\",predictedSentiment[5])\n",
    "\n",
    "import operator\n",
    "index, value = max(enumerate(predictedSentiment), key=operator.itemgetter(1))\n",
    "\n",
    "labelPrediction[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
