{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as j\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "json_data = None\n",
    "with open('question.json', 'r') as f:\n",
    "     json_data = j.load(f)\n",
    "\n",
    "data = pd.DataFrame(json_data)\n",
    "\n",
    "test_data = None\n",
    "\n",
    "with open('question.json', 'r') as f:\n",
    "     test_data = j.load(f)\n",
    "\n",
    "test_data = pd.DataFrame(test_data)\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "words = stopwords.words(\"english\")\n",
    "\n",
    "data['cleaned'] = data['question'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "test_data['cleaned'] = test_data['question'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'everi time i imagin someon i love i could contact serious ill even death'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json as j\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import csv\n",
    "\n",
    "data = pd.read_csv('iseardataset.csv')\n",
    "test_data = data\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "words = stopwords.words(\"english\")\n",
    "\n",
    "data['cleaned'] = data['text'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "test_data['cleaned'] = data['text'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n",
    "\n",
    "data['cleaned'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "SeqLength = 42\n",
    "counter = 0\n",
    "for p in wordsList:\n",
    "    counter = counter+1\n",
    "    if counter >= SeqLength:                \n",
    "        break\n",
    "        \n",
    "wordVectors = np.load('wordVectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeltext = data['label']\n",
    "\n",
    "labeltext1= data['label']\n",
    "\n",
    "\n",
    "testdata = data['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueLabel=[]\n",
    "for w in labeltext:\n",
    "    if w not in uniqueLabel:\n",
    "        uniqueLabel.append(w)\n",
    "        \n",
    "uniqueLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7516"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter=[]\n",
    "for word in uniqueLabel:\n",
    "    count=0\n",
    "    for word1 in labeltext:\n",
    "        if word==word1:\n",
    "            count=count+1\n",
    "    counter.append(count)\n",
    "    \n",
    "sum(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on day i feel close partner friend when i feel peac also experi close contact peopl i regard great'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = data['cleaned']\n",
    "\n",
    "\n",
    "len(list1)\n",
    "\n",
    "\n",
    "list1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files is 7516\n",
      "The total number of words in the files is 91532\n",
      "The average number of words in the files is 12.178286322511974\n"
     ]
    }
   ],
   "source": [
    "numWords=[]\n",
    "x = []\n",
    "for pf in list1:\n",
    "    counter = len(pf.split())\n",
    "    x.append(counter)\n",
    "    numWords.append(counter)\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter = 0\n",
    "for pf in list1:\n",
    "    indexCounter = 0\n",
    "    cleanedLine = cleanSentences(pf)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        try:\n",
    "            ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "        indexCounter = indexCounter + 1\n",
    "        if indexCounter >= maxSeqLength:\n",
    "            break\n",
    "    fileCounter = fileCounter + 1 \n",
    "    \n",
    "np.save('idsMatrixQ1', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter = 0\n",
    "for pf in testdata:\n",
    "    indexCounter = 0\n",
    "    cleanedLine = cleanSentences(pf)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        try:\n",
    "            ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "        indexCounter = indexCounter + 1\n",
    "        if indexCounter >= maxSeqLength:\n",
    "            break\n",
    "    fileCounter = fileCounter + 1 \n",
    "    \n",
    "np.save('idsMatrixQ2', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    61,     41,    998,   6710, 399999,     41,  90634,    530,\n",
       "           41,   2716, 399999,  86821,   1927,   1671,     79, 399999,\n",
       "        11056,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids1 = np.load('idsMatrixQ1.npy')\n",
    "\n",
    "\n",
    "ids1[41]\n",
    "\n",
    "\n",
    "ids = np.load('idsMatrixQ2.npy')\n",
    "ids[0]\n",
    "\n",
    "ids[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([1, 2])\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "        print(\"train:\", train_index, \"validation:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,7515)\n",
    "        labeltxt = labeltext[num]\n",
    "        if labeltxt==\"joy\":\n",
    "            labels.append([1,0,0,0,0,0,0])\n",
    "        if labeltxt==\"fear\":\n",
    "            labels.append([0,1,0,0,0,0,0])\n",
    "        if labeltxt==\"anger\":\n",
    "            labels.append([0,0,1,0,0,0,0])\n",
    "        if labeltxt==\"sadness\":\n",
    "            labels.append([0,0,0,1,0,0,0])\n",
    "        if labeltxt==\"disgust\":\n",
    "            labels.append([0,0,0,0,1,0,0])\n",
    "        if labeltxt==\"shame\":\n",
    "            labels.append([0,0,0,0,0,1,0])\n",
    "        if labeltxt==\"guilt\":\n",
    "            labels.append([0,0,0,0,0,0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(1,7515)\n",
    "        labeltxt = labeltext1[num]\n",
    "        if labeltxt==\"joy\":\n",
    "            labels.append([1,0,0,0,0,0,0])\n",
    "        if labeltxt==\"fear\":\n",
    "            labels.append([0,1,0,0,0,0,0])\n",
    "        if labeltxt==\"anger\":\n",
    "            labels.append([0,0,1,0,0,0,0])\n",
    "        if labeltxt==\"sadness\":\n",
    "            labels.append([0,0,0,1,0,0,0])\n",
    "        if labeltxt==\"disgust\":\n",
    "            labels.append([0,0,0,0,1,0,0])\n",
    "        if labeltxt==\"shame\":\n",
    "            labels.append([0,0,0,0,0,1,0])\n",
    "        if labeltxt==\"guilt\":\n",
    "            labels.append([0,0,0,0,0,0,1])\n",
    "        arr[i] = ids1[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "lstmUnits = 10\n",
    "numClasses = 7\n",
    "iterations = 100000\n",
    "numDimensions = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(64), Dimension(24), Dimension(50)])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable rnn/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Vinay Varekar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\Vinay Varekar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Vinay Varekar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-24b8ea197ad3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlstmCell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstmUnits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlstmCell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropoutWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlstmCell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_keep_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.75\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstmCell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlstmUnits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    627\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m         dtype=dtype)\n\u001b[0m\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    818\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 820\u001b[1;33m       swap_memory=swap_memory)\n\u001b[0m\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m   \u001b[1;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\u001b[0m\n\u001b[0;32m   2932\u001b[0m         swap_memory=swap_memory)\n\u001b[0;32m   2933\u001b[0m     \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2934\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2935\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2718\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2719\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[1;32m-> 2720\u001b[1;33m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[0;32m   2721\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2722\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[1;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2660\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m   2661\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2662\u001b[1;33m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2663\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2664\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   2911\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[0;32m   2912\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[1;32m-> 2913\u001b[1;33m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2915\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_eager_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[1;34m(time, output_ta_t, state)\u001b[0m\n\u001b[0;32m    793\u001b[0m           skip_conditionals=True)\n\u001b[0;32m    794\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m       \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[1;31m# Pack state if using state tuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m     \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                              \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recurrent_input_noise\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                              self._input_keep_prob)\n\u001b[1;32m-> 1042\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_should_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_keep_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m       \u001b[1;31m# Identify which subsets of the state to perform dropout on and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;31m# method.  See the class docstring for more details.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n\u001b[1;32m--> 292\u001b[1;33m                                      *args, **kwargs)\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    634\u001b[0m               \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m           \u001b[1;31m# Note: not all sub-classes of Layer call Layer.__init__ (especially\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, inputs_shape)\u001b[0m\n\u001b[0;32m    535\u001b[0m     self._kernel = self.add_variable(\n\u001b[0;32m    536\u001b[0m         \u001b[0m_WEIGHTS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m         shape=[input_depth + h_depth, 4 * self._num_units])\n\u001b[0m\u001b[0;32m    538\u001b[0m     self._bias = self.add_variable(\n\u001b[0;32m    539\u001b[0m         \u001b[0m_BIAS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36madd_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner)\u001b[0m\n\u001b[0;32m    502\u001b[0m                                    \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m                                    \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m                                    partitioner=partitioner)\n\u001b[0m\u001b[0;32m    505\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m           if (trainable and self.trainable\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1260\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1262\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1263\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1264\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1095\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1097\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    433\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    741\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 743\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    744\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable rnn/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\Vinay Varekar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\Vinay Varekar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Vinay Varekar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "print(prediction)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "print(correctPred)\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "print(accuracy)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models1/pretrained_lstm.ckpt-10000\n",
      "saved to models1/pretrained_lstm.ckpt-20000\n",
      "saved to models1/pretrained_lstm.ckpt-30000\n",
      "saved to models1/pretrained_lstm.ckpt-40000\n",
      "saved to models1/pretrained_lstm.ckpt-50000\n",
      "saved to models1/pretrained_lstm.ckpt-60000\n",
      "saved to models1/pretrained_lstm.ckpt-70000\n",
      "saved to models1/pretrained_lstm.ckpt-80000\n",
      "saved to models1/pretrained_lstm.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "# sess = tf.InteractiveSession()\n",
    "# saver = tf.train.Saver()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# for i in range(iterations):\n",
    "#    #Next Batch of reviews\n",
    "#    nextBatch, nextBatchLabels = getTrainBatch()\n",
    "#    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "#    #Write summary to Tensorboard\n",
    "#    if (i % 50 == 0):\n",
    "#        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "#        writer.add_summary(summary, i)\n",
    "\n",
    "#    #Save the network every 10,000 training iterations\n",
    "#    if (i % 10000 == 0 and i != 0):\n",
    "#        save_path = saver.save(sess, \"models1/pretrained_lstm.ckpt\", global_step=i)\n",
    "#        print(\"saved to %s\" % save_path)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models1\\pretrained_lstm.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 56.25\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 57.8125\n",
      "Accuracy for this batch: 53.125\n",
      "Accuracy for this batch: 60.9375\n",
      "Accuracy for this batch: 54.6875\n",
      "Accuracy for this batch: 64.0625\n",
      "Accuracy for this batch: 68.75\n",
      "Accuracy for this batch: 65.625\n",
      "Accuracy for this batch: 54.6875\n",
      "Accuracy for this batch: 54.6875\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 75.0\n",
      "Accuracy for this batch: 53.125\n",
      "Accuracy for this batch: 60.9375\n",
      "Accuracy for this batch: 68.75\n",
      "Accuracy for this batch: 65.625\n",
      "Accuracy for this batch: 60.9375\n",
      "Accuracy for this batch: 51.5625\n",
      "Accuracy for this batch: 54.6875\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 57.8125\n",
      "Accuracy for this batch: 51.5625\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 64.0625\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 65.625\n",
      "Accuracy for this batch: 56.25\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 68.75\n",
      "Accuracy for this batch: 56.25\n",
      "Accuracy for this batch: 59.375\n",
      "Accuracy for this batch: 65.625\n",
      "Accuracy for this batch: 54.6875\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 56.25\n",
      "Accuracy for this batch: 67.1875\n",
      "Accuracy for this batch: 53.125\n",
      "Accuracy for this batch: 60.9375\n",
      "Accuracy for this batch: 71.875\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 67.1875\n",
      "Accuracy for this batch: 64.0625\n",
      "Accuracy for this batch: 48.4375\n",
      "Accuracy for this batch: 60.9375\n",
      "Accuracy for this batch: 57.8125\n",
      "Accuracy for this batch: 57.8125\n",
      "Accuracy for this batch: 53.125\n",
      "Accuracy for this batch: 57.8125\n"
     ]
    }
   ],
   "source": [
    "iterations = 50\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"When I was not accepted as a student in finance and accounting.\"\n",
    "inputMatrix = getSentenceMatrix(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.6381736\n",
      "fear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.31307772,  2.6381736 , -1.40887   , -0.36780676,  0.7357974 ,\n",
       "       -1.0194211 ,  0.07250335], dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "max1 = 0\n",
    "count = 0\n",
    "for i in range(0,6):\n",
    "    if predictedSentiment[max1] < predictedSentiment[i]:\n",
    "        max1 = i\n",
    "        count = predictedSentiment[i]\n",
    "\n",
    "print(max1,count)\n",
    "print(uniqueLabel[max1])\n",
    "predictedSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"When one lets friends down\"\n",
    "inputMatrix = getSentenceMatrix(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.57973254\n",
      "sadness\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.17515756, -0.5884043 , -0.58662677,  0.57973254,  0.43536714,\n",
       "        0.01479686, -0.09924237], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "max1 = 0\n",
    "count = 0\n",
    "for i in range(0,6):\n",
    "    if predictedSentiment[max1] < predictedSentiment[i]:\n",
    "        max1 = i\n",
    "        count = predictedSentiment[i]\n",
    "\n",
    "print(max1,count)\n",
    "print(uniqueLabel[max1])\n",
    "predictedSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"When the whole family gets together for a one week holiday.\"\n",
    "inputMatrix = getSentenceMatrix(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9278729\n",
      "fear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.2347897 ,  0.9278729 , -0.7285969 ,  0.27474755,  0.62038076,\n",
       "       -0.24732117,  0.1686889 ], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "max1 = 0\n",
    "count = 0\n",
    "for i in range(0,6):\n",
    "    if predictedSentiment[max1] < predictedSentiment[i]:\n",
    "        max1 = i\n",
    "        count = predictedSentiment[i]\n",
    "\n",
    "print(max1,count)\n",
    "print(uniqueLabel[max1])\n",
    "predictedSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Diabetes Housing dataset\n",
    "columns = \"age sex bmi map tc ldl hdl tch ltg glu\".split() # Declare the columns names\n",
    "diabetes = datasets.load_diabetes() # Call the diabetes dataset from sklearn\n",
    "df = pd.DataFrame(diabetes.data, columns=columns) # load the dataset as a pandas data frame\n",
    "y = diabetes.target # define the target variable (dependent variable) as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10) (353,)\n",
      "(89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train, y_train)\n",
    "predictions = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([123.94912283, 178.75109307,  58.732693  , 120.32302802,\n",
       "       189.18183373])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Predictions')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## The line / model\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5690116201581218\n"
     ]
    }
   ],
   "source": [
    "print(\"Score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=2, random_state=None, shuffle=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=2, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold # import KFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) # create an array\n",
    "y = np.array([1, 2, 3, 4]) # Create another array\n",
    "kf = KFold(n_splits=2) # Define the split - into 2 folds \n",
    "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "print(kf) \n",
    "KFold(n_splits=2, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN: [0 1] TEST: [2 3]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    " print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    " X_train, X_test = X[train_index], X[test_index]\n",
    " y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [1] TEST: [0]\n",
      "[[3 4]] [[1 2]] [2] [1]\n",
      "TRAIN: [0] TEST: [1]\n",
      "[[1 2]] [[3 4]] [1] [2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut \n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([1, 2])\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(X)\n",
    "\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "   X_train, X_test = X[train_index], X[test_index]\n",
    "   y_train, y_test = y[train_index], y[test_index]\n",
    "   print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinay Varekar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports: \n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [442, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-174-900b18b2ccee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Perform 6-fold cross validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cross-validated scores:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m   1568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m     \"\"\"\n\u001b[1;32m-> 1570\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [442, 2]"
     ]
    }
   ],
   "source": [
    "# Perform 6-fold cross validation\n",
    "scores = cross_val_score(model, df, y, cv=6)\n",
    "print(\"Cross-validated scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
